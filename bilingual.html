<!doctype html>
<html lang="zh-Hant"><head><meta charset="utf-8">
<title>True/False Ch1â€“Ch4 ä¸­è‹±å°ç…§</title>

<style>
  :root{--bg:#f6f7fb;--ink:#111;--muted:#555;--card:#fff;--accent:#2b6cb0}
  html,body{margin:0;padding:0;background:var(--bg);color:var(--ink);font-family:system-ui,-apple-system,Segoe UI,Noto Sans TC,Microsoft JhengHei,Arial,sans-serif}
  .container{max-width:1100px;margin:24px auto;padding:0 16px}
  h1{font-size:22px;margin:8px 0 16px}
  .card{background:var(--card);border-radius:10px;box-shadow:0 6px 18px rgba(0,0,0,.06);padding:14px;margin:12px 0;border:1px solid #e5e7eb}
  .qnum{font-weight:800;color:#1f2937;margin-bottom:6px}
  .row{display:grid;grid-template-columns:1fr;gap:10px}
  .sec h3{font-size:14px;margin:4px 0;color:#0f172a}
  .en,.zh{white-space:pre-wrap}
  .ans{font-weight:700;margin:6px 0}
  .ans .pill{display:inline;font-size:20px;margin-right:0}
  .ans .pill.true{color:#16a34a}
  .ans .pill.false{color:#dc2626}
  .exp{color:var(--muted)}
  .toolbar{display:flex;gap:10px;align-items:center;margin:8px 0 18px}
  .btn{background:var(--accent);color:#fff;border:none;border-radius:8px;padding:10px 14px;font-size:14px;cursor:pointer}
  .btn:hover{filter:brightness(.95)}
  @media print{.toolbar{display:none}.container{max-width:100%;padding:0 10mm}.card{break-inside:avoid;page-break-inside:avoid;border:1px solid #ddd;box-shadow:none}h1{font-size:16pt}}
</style>
<script>
  function toPDF(){ window.print(); }
</script>
<!-- MathJax for inline LaTeX like \( ... \) and $$ ... $$ -->
<script>
  window.MathJax = {tex: {inlineMath: [['\\(','\\)'], ['$', '$']]}, svg: {fontCache: 'global'}};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
<!-- Mobile UX Enhancements: viewport, math, print button styles -->
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
<style>
  html { font-size: clamp(16px, 2.5vw, 18px); }
  body { line-height: 1.6; -webkit-text-size-adjust: 100%; }
  .mjx-container { font-size: clamp(1.05em, 2.6vw, 1.25em) !important; }
  .mjx-container [style*="white-space:nowrap"] { white-space: normal !important; }
  .mjx-container mjx-mrow { display: inline-block; max-width: 100%; overflow-wrap: anywhere; }
  .print-btn {
    position: fixed; right: 16px; bottom: 16px;
    width: 56px; height: 56px; min-width: 44px; min-height: 44px;
    border-radius: 999px; border: none; cursor: pointer;
    background: #1f6feb; color: #fff; font-size: 22px; line-height: 1;
    box-shadow: 0 4px 12px rgba(0,0,0,0.2); z-index: 1000;
  }
  .print-btn:active { transform: scale(0.98); }
  @media (max-width: 480px) { .print-btn { width: 64px; height: 64px; font-size: 24px; right: 14px; bottom: 14px; } }
  @media print { .print-btn { display: none !important; } }
</style>
<script>
  window.MathJax = {
    chtml: { scale: 1.1, linebreaks: { automatic: true, width: 'container' } },
    svg:   { scale: 1.1, linebreaks: { automatic: true, width: 'container' } }
  };
</script>
</head><body>
<div class="container">
<h1>True/False Questions (Ch1â€“Ch4) ä¸­è‹±å°ç…§</h1>
<div class="toolbar"><button class="btn" onclick="toPDF()">åˆ—å° / åŒ¯å‡º PDF</button></div>

<div class="card">
<div class="qnum">Q1</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Maximum Likelihood Estimation (MLE) chooses parameters that maximize the probability of the observed data.</div>
<div class="zh">æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆï¼ˆMLEï¼‰æœƒé¸æ“‡ä½¿è§€æ¸¬è³‡æ–™æ©Ÿç‡æœ€å¤§çš„åƒæ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">MLE finds Î¸ that maximizes \( p(X \mid \theta) \).</div>
<div class="exp">MLE æ‰¾åˆ°èƒ½ä½¿ \( p(X \mid \theta) \) æœ€å¤§çš„åƒæ•¸ Î¸ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q2</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">The log-likelihood is often used instead of likelihood because it avoids numerical underflow and simplifies derivatives.</div>
<div class="zh">å°æ•¸ä¼¼ç„¶å¸¸è¢«ç”¨ä¾†å–ä»£ä¼¼ç„¶ï¼Œå› ç‚ºå®ƒèƒ½é¿å…æ•¸å€¼ä¸‹æº¢ä¸¦ç°¡åŒ–å¾®åˆ†ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Log converts products to sums and stabilizes computation.</div>
<div class="exp">å°æ•¸æœƒæŠŠä¹˜ç©è½‰ç‚ºåŠ ç¸½ï¼Œä¸¦è®“è¨ˆç®—æ›´ç©©å®šã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q3</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">For a Bernoulli distribution, the MLE of the success probability p is the sum of successes divided by the sample size.</div>
<div class="zh">å°æ–¼ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ŒæˆåŠŸæ©Ÿç‡ p çš„ MLE æ˜¯æˆåŠŸæ¬¡æ•¸é™¤ä»¥æ¨£æœ¬æ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">\( \hat{p} = \frac{\sum x_t}{N} \).</div>
<div class="exp">\( \hat{p} = \frac{\sum x_t}{N} \)ï¼Œä¹Ÿå°±æ˜¯æ¨£æœ¬ä¸­çš„æˆåŠŸæ¯”ä¾‹ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q4</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In a multinomial distribution, the probabilities p_i estimated by MLE must sum to less than 1.</div>
<div class="zh">åœ¨å¤šé …åˆ†å¸ƒä¸­ï¼ŒMLE ä¼°è¨ˆå¾—åˆ°çš„å„é¡åˆ¥æ©Ÿç‡ p_i ç¸½å’Œå¿…é ˆå°æ–¼ 1ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">They must sum to exactly 1.</div>
<div class="exp">å„é¡åˆ¥æ©Ÿç‡çš„ç¸½å’Œå¿…é ˆæ­£å¥½ç­‰æ–¼ 1ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q5</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">For a Gaussian distribution, the MLE of the mean is the sample mean.</div>
<div class="zh">å°æ–¼é«˜æ–¯åˆ†å¸ƒï¼Œå¹³å‡æ•¸çš„ MLE å°±æ˜¯æ¨£æœ¬å¹³å‡å€¼ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">\( \hat{\mu} = \frac{1}{N} \sum x_t \).</div>
<div class="exp">\( \hat{\mu} = \frac{1}{N} \sum x_t \)ï¼Œä¹Ÿå°±æ˜¯æ¨£æœ¬å‡å€¼ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q6</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">For a Gaussian distribution, the MLE of variance is biased because it divides by Nâˆ’1.</div>
<div class="zh">å°æ–¼é«˜æ–¯åˆ†å¸ƒï¼ŒMLE çš„è®Šç•°æ•¸ä¼°è¨ˆæœ‰åï¼Œå› ç‚ºå®ƒé™¤ä»¥ Nâˆ’1ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">MLE divides by N. Using Nâˆ’1 gives the unbiased estimator.</div>
<div class="exp">MLE çš„åˆ†æ¯æ˜¯ Nï¼›ç”¨ Nâˆ’1 æ‰æ˜¯ä¸åä¼°è¨ˆã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q7</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Bias measures the difference between the expected value of an estimator and the true parameter.</div>
<div class="zh">åå·®ï¼ˆBiasï¼‰æ˜¯ä¼°è¨ˆé‡çš„æœŸæœ›å€¼èˆ‡çœŸå¯¦åƒæ•¸çš„å·®è·ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Bias = \( E[\hat{\theta}] - \theta \).</div>
<div class="exp">åå·®å®šç¾©ç‚º \( E[\hat{\theta}] - \theta \)ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q8</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Variance measures the squared difference between the estimator and the true parameter.</div>
<div class="zh">è®Šç•°æ•¸è¡¡é‡ä¼°è¨ˆé‡èˆ‡çœŸå¯¦åƒæ•¸çš„å¹³æ–¹å·®ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Variance measures deviation from the estimatorâ€™s mean, not from the true parameter.</div>
<div class="exp">è®Šç•°æ•¸è¡¡é‡çš„æ˜¯ä¼°è¨ˆé‡èˆ‡å…¶å¹³å‡å€¼çš„åé›¢ç¨‹åº¦ï¼Œè€Œéèˆ‡çœŸå¯¦åƒæ•¸çš„å·®è·ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q9</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Mean squared error (MSE) can be decomposed into BiasÂ² plus Variance.</div>
<div class="zh">å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰å¯ä»¥åˆ†è§£ç‚ºåå·®å¹³æ–¹åŠ ä¸Šè®Šç•°æ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">\( MSE = Bias^2 + Variance \).</div>
<div class="exp">\( MSE = Bias^2 + Variance \)ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q10</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Maximum a Posteriori (MAP) estimation includes prior information, unlike MLE.</div>
<div class="zh">æœ€å¤§å¾Œé©—ä¼°è¨ˆï¼ˆMAPï¼‰æœƒç´å…¥å…ˆé©—è³‡è¨Šï¼Œä¸åŒæ–¼ MLEã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">MAP maximizes \( p(\theta \mid X) \), incorporating priors.</div>
<div class="exp">MAP æœ€å¤§åŒ– \( p(\theta \mid X) \)ï¼Œè€Œ MLE æœ€å¤§åŒ– \( p(X \mid \theta) \)ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q11</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In Bayesian estimation, the posterior mean is always equal to the MAP estimate.</div>
<div class="zh">åœ¨è²è‘‰æ–¯ä¼°è¨ˆä¸­ï¼Œå¾Œé©—å¹³å‡å€¼ç¸½æ˜¯ç­‰æ–¼ MAPã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Only under symmetric posteriors with specific priors (e.g., Gaussian).</div>
<div class="exp">åªæœ‰åœ¨å°ç¨±çš„å¾Œé©—åˆ†å¸ƒä¸”å…·æœ‰ç‰¹å®šå…ˆé©—ï¼ˆå¦‚é«˜æ–¯åˆ†å¸ƒï¼‰æ™‚æ‰æœƒç›¸ç­‰ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q12</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Parametric classification assumes a distributional form for each class.</div>
<div class="zh">åƒæ•¸å¼åˆ†é¡å‡è¨­æ¯å€‹é¡åˆ¥çš„åˆ†å¸ƒå½¢å¼ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">For example, Gaussian class-conditional densities.</div>
<div class="exp">ä¾‹å¦‚ï¼Œé«˜æ–¯æ¢ä»¶åˆ†å¸ƒã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q13</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">When two Gaussian classes have equal variances, the decision boundary lies halfway between their means.</div>
<div class="zh">ç•¶å…©å€‹é«˜æ–¯é¡åˆ¥å…·æœ‰ç›¸åŒè®Šç•°æ•¸æ™‚ï¼Œæ±ºç­–é‚Šç•Œä½æ–¼å‡å€¼çš„ä¸­é»ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Equal variance leads to a linear boundary at midpoint.</div>
<div class="exp">ç›¸åŒè®Šç•°æ•¸æœƒä½¿é‚Šç•Œè½åœ¨å‡å€¼çš„ä¸­é»ä¸¦å‘ˆç·šæ€§ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q14</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Increasing model complexity always reduces both bias and variance.</div>
<div class="zh">å¢åŠ æ¨¡å‹è¤‡é›œåº¦ç¸½æœƒåŒæ™‚é™ä½åå·®èˆ‡è®Šç•°æ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Bias decreases but variance increases â†’ bias-variance dilemma.</div>
<div class="exp">åå·®æœƒä¸‹é™ï¼Œä½†è®Šç•°æ•¸æœƒä¸Šå‡ â†’ åå·®-è®Šç•°æ•¸å›°å¢ƒã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q15</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Cross-validation helps measure a modelâ€™s training error.</div>
<div class="zh">äº¤å‰é©—è­‰å¹«åŠ©è¡¡é‡æ¨¡å‹çš„è¨“ç·´èª¤å·®ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It measures generalization error using unseen data.</div>
<div class="exp">å®ƒå¯¦éš›ä¸Šè¡¡é‡çš„æ˜¯æ³›åŒ–èª¤å·®ï¼Œç”¨çš„æ˜¯æœªè¦‹éçš„è³‡æ–™ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q16</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Bayesâ€™ rule combines prior, likelihood, and evidence to compute the posterior probability.</div>
<div class="zh">è²è‘‰æ–¯æ³•å‰‡çµåˆå…ˆé©—ã€ä¼¼ç„¶å’Œè­‰æ“šä¾†è¨ˆç®—å¾Œé©—æ©Ÿç‡ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Posterior = (Likelihood Ã— Prior) / Evidence.</div>
<div class="exp">å¾Œé©— = (ä¼¼ç„¶ Ã— å…ˆé©—) / è­‰æ“šã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q17</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In a binary classification problem, predicting the more probable class minimizes the 0/1 loss.</div>
<div class="zh">åœ¨äºŒåˆ†é¡å•é¡Œä¸­ï¼Œé æ¸¬æ©Ÿç‡è¼ƒå¤§çš„é¡åˆ¥å¯æœ€å°åŒ– 0/1 æå¤±ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Under 0/1 loss, minimum risk corresponds to maximum posterior probability.</div>
<div class="exp">åœ¨ 0/1 æå¤±ä¸‹ï¼Œæœ€å°é¢¨éšªå°æ‡‰æ–¼æœ€å¤§å¾Œé©—æ©Ÿç‡ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q18</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">The prior probability depends on the observed data.</div>
<div class="zh">å…ˆé©—æ©Ÿç‡ä¾è³´æ–¼è§€æ¸¬è³‡æ–™ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">The prior is independent of the data; it reflects beliefs before seeing data.</div>
<div class="exp">å…ˆé©—æ©Ÿç‡èˆ‡è³‡æ–™ç„¡é—œï¼Œå®ƒæ˜¯åœ¨è§€æ¸¬ä¹‹å‰çš„ä¿¡å¿µã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q19</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">When K > 2 classes, Bayesâ€™ decision rule is to choose the class with the maximum posterior probability.</div>
<div class="zh">ç•¶ K > 2 é¡æ™‚ï¼Œè²è‘‰æ–¯æ±ºç­–è¦å‰‡æ˜¯é¸æ“‡å¾Œé©—æ©Ÿç‡æœ€å¤§çš„é¡åˆ¥ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Argmax rule applies for multiple classes.</div>
<div class="exp">Argmax è¦å‰‡é©ç”¨æ–¼å¤šé¡æƒ…æ³ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q20</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Expected risk is the average loss over all possible states, weighted by their posterior probabilities.</div>
<div class="zh">æœŸæœ›é¢¨éšªæ˜¯æ‰€æœ‰å¯èƒ½ç‹€æ…‹ä¸‹çš„å¹³å‡æå¤±ï¼Œä¾å¾Œé©—æ©Ÿç‡åŠ æ¬Šã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">\( R(\alpha \mid x) = \sum_k \lambda(\alpha \mid C_k) P(C_k \mid x) \).</div>
<div class="exp">\( R(\alpha \mid x) = \sum_k \lambda(\alpha \mid C_k) P(C_k \mid x) \)ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q21</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In 0/1 loss, the loss is 1 for correct classification and 0 for misclassification.</div>
<div class="zh">åœ¨ 0/1 æå¤±ä¸‹ï¼Œåˆ†é¡æ­£ç¢ºçš„æå¤±æ˜¯ 1ï¼ŒéŒ¯èª¤çš„æå¤±æ˜¯ 0ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It is 0 if correct, 1 if misclassified.</div>
<div class="exp">æ­£ç¢ºåˆ†é¡æ™‚æå¤±æ˜¯ 0ï¼ŒéŒ¯èª¤åˆ†é¡æ™‚æå¤±æ˜¯ 1ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q22</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Introducing a â€œreject optionâ€ allows the system to avoid making risky decisions when posterior probabilities are uncertain.</div>
<div class="zh">å¼•å…¥ã€Œæ‹’åˆ¤é¸é …ã€å¯ä»¥åœ¨å¾Œé©—æ©Ÿç‡ä¸ç¢ºå®šæ™‚é¿å…é«˜é¢¨éšªæ±ºç­–ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Reject is chosen if all posterior probabilities are too close or below threshold.</div>
<div class="exp">ç•¶æ‰€æœ‰å¾Œé©—æ©Ÿç‡éƒ½éæ–¼æ¥è¿‘æˆ–ä½æ–¼é–€æª»æ™‚ï¼Œå¯é¸æ“‡æ‹’åˆ¤ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q23</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">With unequal losses, the decision rule is still to choose the class with maximum posterior probability.</div>
<div class="zh">ç•¶æå¤±ä¸ç›¸ç­‰æ™‚ï¼Œæ±ºç­–è¦å‰‡ä»ç„¶åªéœ€é¸æ“‡å¾Œé©—æ©Ÿç‡æœ€å¤§çš„é¡åˆ¥ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">With unequal losses, the rule is to minimize expected risk, not just maximize posterior.</div>
<div class="exp">åœ¨ä¸ç­‰æå¤±ä¸‹ï¼Œæ±ºç­–æ‡‰è©²æœ€å°åŒ–æœŸæœ›é¢¨éšªï¼Œè€Œä¸æ˜¯å–®ç´”é¸æœ€å¤§å¾Œé©—ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q24</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Discriminant functions provide a convenient way to implement Bayesâ€™ decision rule.</div>
<div class="zh">åˆ¤åˆ¥å‡½æ•¸æä¾›äº†ä¸€ç¨®æ–¹ä¾¿çš„æ–¹æ³•ä¾†å¯¦ç¾è²è‘‰æ–¯æ±ºç­–è¦å‰‡ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Class is assigned to the region with the maximum discriminant function.</div>
<div class="exp">åˆ†é¡æœƒè¢«æŒ‡æ´¾åˆ°åˆ¤åˆ¥å‡½æ•¸å€¼æœ€å¤§çš„å€åŸŸã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q25</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">For two classes, the discriminant can be expressed as the log odds of the posterior probabilities.</div>
<div class="zh">å°æ–¼äºŒåˆ†é¡ï¼Œåˆ¤åˆ¥å‡½æ•¸å¯ä»¥å¯«æˆå¾Œé©—æ©Ÿç‡çš„å°æ•¸å‹ç®—ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">\( g(x) = \log \frac{P(C_1 \mid x)}{P(C_2 \mid x)} \).</div>
<div class="exp">åˆ¤åˆ¥å‡½æ•¸å…¬å¼ç‚º \( g(x) = \log \frac{P(C_1 \mid x)}{P(C_2 \mid x)} \)ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q26</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">If losses are equal for all misclassifications, the Bayes classifier reduces to the MAP rule.</div>
<div class="zh">è‹¥æ‰€æœ‰èª¤åˆ†é¡æå¤±ç›¸ç­‰ï¼Œè²è‘‰æ–¯åˆ†é¡å™¨ç°¡åŒ–ç‚º MAP è¦å‰‡ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Under equal 0/1 loss, choose the class with maximum posterior.</div>
<div class="exp">åœ¨ç›¸ç­‰çš„ 0/1 æå¤±ä¸‹ï¼Œé¸æ“‡æœ€å¤§å¾Œé©—æ©Ÿç‡çš„é¡åˆ¥ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q27</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">The Bayes classifier always guarantees zero classification error.</div>
<div class="zh">è²è‘‰æ–¯åˆ†é¡å™¨ç¸½èƒ½ä¿è­‰é›¶åˆ†é¡éŒ¯èª¤ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It minimizes error, but cannot eliminate it if class distributions overlap.</div>
<div class="exp">å®ƒèƒ½æœ€å°åŒ–éŒ¯èª¤ï¼Œä½†è‹¥é¡åˆ¥åˆ†å¸ƒæœ‰é‡ç–Šï¼ŒéŒ¯èª¤ç‡ä»å¤§æ–¼ 0ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q28</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Utility theory is a generalization of risk minimization where the goal is to maximize expected utility.</div>
<div class="zh">æ•ˆç”¨ç†è«–æ˜¯é¢¨éšªæœ€å°åŒ–çš„æ¨å»£ï¼Œç›®æ¨™æ˜¯æœ€å¤§åŒ–æœŸæœ›æ•ˆç”¨ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Risk uses losses, utility theory uses gains.</div>
<div class="exp">é¢¨éšªæ˜¯ä»¥æå¤±è¡¡é‡ï¼Œæ•ˆç”¨ç†è«–å‰‡ä»¥æ”¶ç›Šç‚ºåŸºç¤ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q29</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In Bayesian decision theory, evidence p(x) depends on the choice of class.</div>
<div class="zh">åœ¨è²è‘‰æ–¯æ±ºç­–ç†è«–ä¸­ï¼Œè­‰æ“š \(p(x)\) ä¾è³´æ–¼æ‰€é¸é¡åˆ¥ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Evidence \( p(x) = \sum_k p(x \mid C_k) P(C_k) \) is independent of decision.</div>
<div class="exp">è­‰æ“š \( p(x) = \sum_k p(x \mid C_k) P(C_k) \)ï¼Œèˆ‡æ±ºç­–ç„¡é—œã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q30</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">A dichotomizer refers to a classifier with more than two classes.</div>
<div class="zh">dichotomizer æŒ‡çš„æ˜¯èƒ½è™•ç†å¤šæ–¼å…©é¡çš„åˆ†é¡å™¨ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Dichotomizer is for two classes; polychotomizer is for K > 2.</div>
<div class="exp">dichotomizer æ˜¯äºŒé¡åˆ†é¡å™¨ï¼Œè‹¥ K > 2 å‰‡ç¨±ç‚º polychotomizerã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q31</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Supervised learning requires labeled training data, where each input has a corresponding output label.</div>
<div class="zh">ç›£ç£å¼å­¸ç¿’éœ€è¦æ¨™è¨˜çš„è¨“ç·´è³‡æ–™ï¼Œæ¯å€‹è¼¸å…¥éƒ½æœ‰å°æ‡‰çš„è¼¸å‡ºæ¨™ç±¤ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">By definition, supervised learning uses input-output pairs.</div>
<div class="exp">æ ¹æ“šå®šç¾©ï¼Œç›£ç£å¼å­¸ç¿’ä½¿ç”¨è¼¸å…¥â€”è¼¸å‡ºé…å°ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q32</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Positive and negative examples in classification refer to correctly and incorrectly classified instances.</div>
<div class="zh">åœ¨åˆ†é¡ä¸­ï¼Œæ­£ä¾‹å’Œåä¾‹åˆ†åˆ¥æŒ‡æ­£ç¢ºèˆ‡éŒ¯èª¤åˆ†é¡çš„æ¨£æœ¬ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">They mean belonging or not belonging to the target class, not correctness of prediction.</div>
<div class="exp">å®ƒå€‘æŒ‡çš„æ˜¯æ˜¯å¦å±¬æ–¼ç›®æ¨™é¡åˆ¥ï¼Œè€Œä¸æ˜¯é æ¸¬æ˜¯å¦æ­£ç¢ºã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q33</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">A hypothesis class defines the set of all candidate functions the learner can choose from.</div>
<div class="zh">å‡è¨­ç©ºé–“å®šç¾©äº†å­¸ç¿’å™¨å¯é¸çš„å€™é¸å‡½æ•¸é›†åˆã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">H is the space of possible hypotheses.</div>
<div class="exp">H æ˜¯æ‰€æœ‰å¯èƒ½å‡è¨­çš„ç©ºé–“ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q34</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">The version space consists of all hypotheses in H that are consistent with the training data.</div>
<div class="zh">ç‰ˆæœ¬ç©ºé–“åŒ…å«å‡è¨­ç©ºé–“ä¸­æ‰€æœ‰èˆ‡è¨“ç·´è³‡æ–™ä¸€è‡´çš„å‡è¨­ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Consistent hypotheses agree with all labeled examples.</div>
<div class="exp">ä¸€è‡´çš„å‡è¨­å¿…é ˆèˆ‡æ‰€æœ‰æ¨™è¨˜æ¨£æœ¬ç›¸ç¬¦ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q35</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Margin in classification refers to the number of misclassified points.</div>
<div class="zh">åˆ†é¡ä¸­çš„ margin æŒ‡çš„æ˜¯è¢«éŒ¯åˆ†çš„æ¨£æœ¬æ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Margin is the distance between data points and the decision boundary.</div>
<div class="exp">é–“éš”æ˜¯æ¨£æœ¬é»åˆ°æ±ºç­–é‚Šç•Œçš„è·é›¢ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q36</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">A larger margin generally improves generalization and robustness to noise.</div>
<div class="zh">è¼ƒå¤§çš„ margin ä¸€èˆ¬èƒ½æå‡æ³›åŒ–èƒ½åŠ›ä¸¦å¢å¼·å°é›œè¨Šçš„ç©©å¥æ€§ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Thats why SVM aims for maximum margin.</div>
<div class="exp">é€™å°±æ˜¯ç‚ºä»€éº¼ SVM è¿½æ±‚æœ€å¤§é–“éš”ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q37</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Occamâ€™s Razor suggests preferring more complex models because they fit training data better.</div>
<div class="zh">å¥§å¡å§†å‰ƒåˆ€åŸå‰‡å»ºè­°åå¥½æ›´è¤‡é›œçš„æ¨¡å‹ï¼Œå› ç‚ºå®ƒèƒ½æ›´å¥½åœ°æ“¬åˆè¨“ç·´è³‡æ–™ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It recommends choosing simpler models if performance is comparable.</div>
<div class="exp">å®ƒå»ºè­°åœ¨è¡¨ç¾ç›¸è¿‘æ™‚é¸æ“‡æ›´ç°¡å–®çš„æ¨¡å‹ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q38</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Overfitting occurs when the hypothesis class is too simple compared to the true concept.</div>
<div class="zh">ç•¶å‡è¨­ç©ºé–“éæ–¼ç°¡å–®æ™‚æœƒç™¼ç”Ÿéæ“¬åˆã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">That describes underfitting. Overfitting happens when the model is too complex.</div>
<div class="exp">é‚£æè¿°çš„æ˜¯æ¬ æ“¬åˆï¼›éæ“¬åˆç™¼ç”Ÿåœ¨æ¨¡å‹å¤ªè¤‡é›œæ™‚ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q39</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Learning problems are ill-posed without inductive bias, since multiple hypotheses can explain the data.</div>
<div class="zh">å¦‚æœæ²’æœ‰æ­¸ç´åå¥½ï¼Œå­¸ç¿’å•é¡Œæ˜¯ä¸é©å®šçš„ï¼Œå› ç‚ºå¤šå€‹å‡è¨­éƒ½èƒ½è§£é‡‹è³‡æ–™ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Bias is needed to select one solution.</div>
<div class="exp">éœ€è¦åå·®ä¾†é¸æ“‡å”¯ä¸€è§£ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q40</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Increasing hypothesis complexity always decreases generalization error.</div>
<div class="zh">å¢åŠ å‡è¨­è¤‡é›œåº¦ä¸€å®šæœƒé™ä½æ³›åŒ–èª¤å·®ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Error decreases first, but then increases due to overfitting (bias-variance trade-off).</div>
<div class="exp">èª¤å·®æœƒå…ˆä¸‹é™ï¼Œä½†ä¹‹å¾Œå› éæ“¬åˆè€Œä¸Šå‡ï¼ˆåå·®-è®Šç•°æ•¸æ¬Šè¡¡ï¼‰ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q41</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Cross-validation helps estimate generalization error by testing models on unseen data subsets.</div>
<div class="zh">äº¤å‰é©—è­‰é€éåœ¨æœªä½¿ç”¨æ–¼è¨“ç·´çš„è³‡æ–™å­é›†ä¸Šæ¸¬è©¦æ¨¡å‹ï¼Œä¾†ä¼°è¨ˆæ³›åŒ–èª¤å·®ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Validation/test sets mimic unseen data.</div>
<div class="exp">é©—è­‰/æ¸¬è©¦é›†æ¨¡æ“¬æœªè¦‹éçš„è³‡æ–™ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q42</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">In k-fold cross-validation, the dataset is divided into k parts and each part is used once as validation.</div>
<div class="zh">åœ¨ k æŠ˜äº¤å‰é©—è­‰ä¸­ï¼Œè³‡æ–™è¢«åŠƒåˆ†æˆ k ä»½ï¼Œæ¯ä»½éƒ½æœƒæ­£å¥½ä¸€æ¬¡ä½œç‚ºé©—è­‰é›†ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">This ensures efficient use of limited dataï¹’</div>
<div class="exp">é€™ç¢ºä¿äº†æœ‰é™è³‡æ–™çš„æœ‰æ•ˆåˆ©ç”¨ï¹’</div>
</div>
</div>
<div class="card">
<div class="qnum">Q43</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Mean Squared Error (MSE) is commonly used as the loss function in regression tasks.</div>
<div class="zh">åœ¨å›æ­¸ä»»å‹™ä¸­ï¼Œå‡æ–¹èª¤å·®ï¼ˆMSEï¼‰å¸¸è¢«ç”¨ä½œæå¤±å‡½æ•¸ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">MSE measures squared deviation between predictions and true values.</div>
<div class="exp">MSE è¡¡é‡é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“çš„å¹³æ–¹åå·®ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q44</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Underfitting means the hypothesis class is too complex, leading to poor training performance.</div>
<div class="zh">æ¬ æ“¬åˆè¡¨ç¤ºå‡è¨­ç©ºé–“éæ–¼è¤‡é›œï¼Œå°è‡´è¨“ç·´æ•ˆæœå·®ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Underfitting means the model is too simple to capture patterns.</div>
<div class="exp">æ¬ æ“¬åˆæ˜¯å› ç‚ºæ¨¡å‹å¤ªç°¡å–®ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q45</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Model selection is about choosing the best hypothesis class or model complexity to balance fit and generalization.</div>
<div class="zh">æ¨¡å‹é¸æ“‡æ˜¯ç‚ºäº†é¸å‡ºæœ€ä½³çš„å‡è¨­ç©ºé–“æˆ–è¤‡é›œåº¦ï¼Œä»¥å¹³è¡¡æ“¬åˆèˆ‡æ³›åŒ–ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">The goal is to achieve good predictive performance on new data.</div>
<div class="exp">ç›®æ¨™æ˜¯åœ¨æ–°è³‡æ–™ä¸Šé”åˆ°è‰¯å¥½çš„é æ¸¬è¡¨ç¾ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q46</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Pattern recognition is mainly concerned with tasks such as image, speech, and text classification.</div>
<div class="zh">æ¨¡å¼è­˜åˆ¥ä¸»è¦é—œæ³¨æ–¼å½±åƒã€èªéŸ³å’Œæ–‡å­—çš„åˆ†é¡ä»»å‹™ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">PR originated from signal processing and pattern classification.</div>
<div class="exp">æ¨¡å¼è­˜åˆ¥èµ·æºæ–¼è¨Šè™Ÿè™•ç†èˆ‡æ¨¡å¼åˆ†é¡ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q47</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Machine learning is restricted to recognition tasks and does not include prediction or generation.</div>
<div class="zh">æ©Ÿå™¨å­¸ç¿’åƒ…é™æ–¼è¾¨è­˜ä»»å‹™ï¼Œä¸åŒ…å«é æ¸¬æˆ–ç”Ÿæˆã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">ML generalizes to prediction, reinforcement learning, generative models, etc.</div>
<div class="exp">æ©Ÿå™¨å­¸ç¿’æ¨å»£åˆ°é æ¸¬ã€å¼·åŒ–å­¸ç¿’ã€ç”Ÿæˆæ¨¡å‹ç­‰ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q48</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Face verification is the problem of deciding whether two images belong to the same person.</div>
<div class="zh">äººè‡‰é©—è­‰æ˜¯åˆ¤æ–·å…©å¼µåœ–ç‰‡æ˜¯å¦å±¬æ–¼åŒä¸€å€‹äººã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It compares image pairs, unlike identification.</div>
<div class="exp">å®ƒæ¯”è¼ƒå½±åƒå°ï¼Œä¸åŒæ–¼è­˜åˆ¥ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q49</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Face detection aims to determine the identity of the face in an image.</div>
<div class="zh">äººè‡‰æª¢æ¸¬çš„ç›®æ¨™æ˜¯åˆ¤æ–·äººè‡‰çš„èº«ä»½ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Detection only finds the location of faces, not identity.</div>
<div class="exp">åµæ¸¬åªæ‰¾å‡ºäººè‡‰çš„ä½ç½®ï¼Œè€Œä¸æ˜¯èº«ä»½ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q50</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Identification compares an unknown face against a database and finds the closest match.</div>
<div class="zh">è­˜åˆ¥ï¼ˆIdentificationï¼‰æ˜¯å°‡æœªçŸ¥çš„äººè‡‰èˆ‡è³‡æ–™åº«æ¯”å°ï¼Œæ‰¾å‡ºæœ€æ¥è¿‘çš„åŒ¹é…ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Identification/retrieval matches unknowns to knowns.</div>
<div class="exp">è­˜åˆ¥/æª¢ç´¢å°‡æœªçŸ¥èˆ‡å·²çŸ¥é€²è¡ŒåŒ¹é…ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q51</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Pattern recognition typically relies more on handcrafted feature design, while machine learning often employs automatic feature learning.</div>
<div class="zh">æ¨¡å¼è­˜åˆ¥é€šå¸¸è¼ƒä¾è³´äººå·¥è¨­è¨ˆç‰¹å¾µï¼Œè€Œæ©Ÿå™¨å­¸ç¿’å‰‡å¸¸ç”¨è‡ªå‹•ç‰¹å¾µå­¸ç¿’ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">ML (espï¹’ deep learning) reduces manual feature engineering.</div>
<div class="exp">æ©Ÿå™¨å­¸ç¿’ï¼ˆç‰¹åˆ¥æ˜¯æ·±åº¦å­¸ç¿’ï¼‰æ¸›å°‘äº†æ‰‹å‹•ç‰¹å¾µå·¥ç¨‹ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q52</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Template matching is a classic example of machine learning.</div>
<div class="zh">æ¨¡æ¿åŒ¹é…æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å…¸å‹ä¾‹å­ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It is a traditional PR technique without learning capability.</div>
<div class="exp">å®ƒæ˜¯ä¸€ç¨®æ²’æœ‰å­¸ç¿’èƒ½åŠ›çš„å‚³çµ±æ¨¡å¼è­˜åˆ¥æŠ€è¡“ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q53</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Support Vector Machines (SVMs) and k-NN can be used in both pattern recognition and machine learning contexts.</div>
<div class="zh">æ”¯æŒå‘é‡æ©Ÿï¼ˆSVMï¼‰å’Œ k æœ€è¿‘é„°ï¼ˆk-NNï¼‰æ—¢å¯ç”¨æ–¼æ¨¡å¼è­˜åˆ¥ï¼Œä¹Ÿå¯ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">They are shared techniques across both fields.</div>
<div class="exp">å®ƒå€‘æ˜¯å…©å€‹é ˜åŸŸå…±äº«çš„æŠ€è¡“ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q54</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">The decision boundary in classification divides the input space into regions assigned to different classes.</div>
<div class="zh">åœ¨åˆ†é¡ä»»å‹™ä¸­ï¼Œæ±ºç­–é‚Šç•Œå°‡è¼¸å…¥ç©ºé–“åŠƒåˆ†æˆä¸åŒçš„é¡åˆ¥å€åŸŸã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It separates low-risk vs high-risk, or class labels in general.</div>
<div class="exp">å®ƒå°‡ä½é¢¨éšªèˆ‡é«˜é¢¨éšªå€åˆ†é–‹ä¾†ï¼Œæˆ–æ›´ä¸€èˆ¬åœ°ï¼Œå€åˆ†é¡åˆ¥æ¨™ç±¤ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q55</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Regression tasks predict continuous values rather than discrete categories.</div>
<div class="zh">å›æ­¸ä»»å‹™é æ¸¬çš„æ˜¯é€£çºŒå€¼ï¼Œè€Œä¸æ˜¯é›¢æ•£é¡åˆ¥ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Examples include predicting house prices or steering angle.</div>
<div class="exp">ä¾‹å¦‚æˆ¿åƒ¹é æ¸¬æˆ–è‡ªé§•è»Šè½‰å‘è§’åº¦çš„ä¼°è¨ˆã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q56</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Regression models cannot handle multiple outputs simultaneously.</div>
<div class="zh">å›æ­¸æ¨¡å‹ä¸èƒ½åŒæ™‚è™•ç†å¤šå€‹è¼¸å‡ºã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Multi-output regression (e.g., robot arm joint angles) is possible.</div>
<div class="exp">å¤šè¼¸å‡ºå›æ­¸æ˜¯å¯è¡Œçš„ï¼ˆä¾‹å¦‚ï¼šæ©Ÿå™¨æ‰‹è‡‚é—œç¯€è§’åº¦ï¼‰ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q57</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Supervised learning is unsuitable for prediction tasks.</div>
<div class="zh">ç›£ç£å¼å­¸ç¿’ä¸é©åˆç”¨ä¾†åšé æ¸¬ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">Prediction is one of its main purposes.</div>
<div class="exp">é æ¸¬æ˜¯å…¶ä¸»è¦ç›®çš„ä¹‹ä¸€ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q58</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Unsupervised learning does not require labeled outputs and is often used for clustering.</div>
<div class="zh">éç›£ç£å¼å­¸ç¿’ä¸éœ€è¦æ¨™è¨˜è¼¸å‡ºï¼Œå¸¸ç”¨æ–¼åˆ†ç¾¤ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It finds structure in unlabeled data.</div>
<div class="exp">å®ƒåœ¨æœªæ¨™è¨˜è³‡æ–™ä¸­ç™¼ç¾çµæ§‹ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q59</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Response surface design in regression refers to approximating nonlinear relationships between inputs and outputs for optimization.</div>
<div class="zh">å›æ­¸ä¸­çš„éŸ¿æ‡‰é¢è¨­è¨ˆæ˜¯ç”¨ä¾†è¿‘ä¼¼è¼¸å…¥èˆ‡è¼¸å‡ºé–“çš„éç·šæ€§é—œä¿‚ï¼Œä»¥åˆ©æœ€ä½³åŒ–ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill true">â­•ï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It models and optimizes complex response functions.</div>
<div class="exp">å®ƒå°è¤‡é›œçš„éŸ¿æ‡‰å‡½æ•¸é€²è¡Œå»ºæ¨¡èˆ‡æœ€ä½³åŒ–ã€‚</div>
</div>
</div>
<div class="card">
<div class="qnum">Q60</div>
<div class="sec">
<h3>åŸæ–‡é¡Œç›®</h3>
<div class="en">Outlier detection can be considered an application of supervised learning.</div>
<div class="zh">é›¢ç¾¤é»åµæ¸¬å¯ä»¥è¢«è¦–ç‚ºç›£ç£å¼å­¸ç¿’çš„æ‡‰ç”¨ã€‚</div>
</div>
<div class="sec">
<h3>ç­”æ¡ˆ</h3>
<div class="ans"><span class="pill false">âŒï¸</span></div>
</div>
<div class="sec">
<h3>Explanation</h3>
<div class="exp">It is often unsupervised, since anomalies lack labeled training examples.</div>
<div class="exp">å®ƒé€šå¸¸æ˜¯éç›£ç£å¼çš„ï¼Œå› ç‚ºç•°å¸¸é»ç¼ºä¹æ¨™è¨˜çš„è¨“ç·´æ¨£æœ¬ã€‚</div>
</div>
</div>

</div>
<button class="print-btn" id="printBtn" aria-label="åˆ—å°">ğŸ–¨ï¸</button>
<script>
  document.getElementById('printBtn')?.addEventListener('click', () => window.print());
</script>
</body></html>
